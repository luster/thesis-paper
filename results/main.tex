We present results here for mainly shallow network architectures. At the output layer of each network, an identity nonlinearity is used. At any other layer, the modified ReLU (mReLU) is used. Unless otherwise noted, batch normalization is applied at the input layer. Each network is compared first to itself at varying noise levels (-6 dB, -3 dB, 0 dB, 3 dB, 6 dB SNR) in terms of convergence as well as the mean squared error (MSE) for inferences.

Training minibatches consist of 128 examples, each with 1024-sample FFT frames of $\norm{Y[k]}$ at a sampling rate 16 kHz. Time windows are windowed using the Hanning window, and we use 50\% overlap for perfect reconstruction at inference time. The examples used are a sum of sine waves at four fixed frequencies with uniform random amplitude and phase. The frequencies are chosen to form an A4 major chord (1-3-5-8) at slightly de-tuned frequencies so as not to allow the network to learn any pattern from the immediate harmonic structure.

\begin{align}
f &= [441, 549, 660, 881]\qquad \text{Hz}\\
x[n] &= \sum_{i=0}^{3} A_i \sin{2 \pi f_i / f_s n + \phi_i}, \qquad n=0\ldots ,1023\\
A_i &\sim U(0.25, 0.75)\\
\phi &\sim U(0, 2\pi)
\end{align}

Applied noise is additive-white Gaussian noise (AWGN), with the variance $\sigma^2$ selected to achieve the desired average SNR for each minibatch as in Equation \ref{eq:siggy}.

\begin{align}
N[n] &\sim N(0, \sigma^2), \qquad n=0\ldots ,1023\\
y[n] &= x[n] + N[n]
\end{align}

In semi-supervised cases where we use the soft label $y$ for noise-only versus signal-plus-noise examples, we use 25\% noise-only examples per minibatch. For inference calculations, we construct a minibatch with consecutive overlapping, windowed frames.

Simulations are written in Python 2.7 using Lasasgne \cite{sander_dieleman_2015_27878}, a ``lightweight library to build and train neural networks in Theano.'' Theano is a ``Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.'' \cite{2016arXiv160502688short} Theano boasts parallel GPU support, numpy support (a mathematical Python library), numerical stability, and symbolic differentiation, among other features. These libraries and frameworks allow for ease of developing deep, novel architectures and save time in doing things like calculating gradients, weight updates and back-propagation. Sample simulation code is shown in the Appendix.

Weight updates are calculated using Adam updates \cite{DBLP:journals/corr/KingmaB14}. 2000 iterations (minibatches) are used for each simulation. Unless otherwise noted, each hidden layer uses 2000 hidden nodes.

Loss-based plots show the loss function convergence during training iterations, where the l. Mean squared error plots show the MSE every 50 training examples for an inference example that does not change.

\subsection{Supervised Autoencoder}

The following results show a single-layer autoencoder with and without batch normalization at the input layer.

\subsubsection{Batch Normalized Input}

In Figure \ref{fig:paris-bn-loss}, we can see that the loss function appears to converge at or before 2000 iterations. As expected, as SNR increases, the loss objective converges to a lower value. Since this network is trained using only the squared error loss, this should be expected. Note that as the SNR increases, the difference in the converged value gets smaller. Also interesting is the fact that lower SNR plots converge more quickly but to higher values. This suggests that the network does not respond well to too much noise.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-loss}
\caption{Loss at various SNRs for Supervised Single-Layer Autoencoder with Batch Normalization at the Input}\label{fig:paris-bn-loss}
\end{figure}

In Figure \ref{fig:paris-bn-mse}, we can see that the MSE generally goes down as SNR goes up. This should be expected, though perhaps there may be an error in the simulation since the lines blur a bit between -3 dB and 6 dB.

Also, the saved audio from which the MSE's were calculated have some distortion introduced from the network.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-mse}
\caption{MSE at various SNRs for Supervised Single-Layer Autoencoder with Batch Normalization at the Input}\label{fig:paris-bn-mse}
\end{figure}

\subsubsection{Non-Batch Normalized Input}

As expected, in Figure \ref{fig:paris-loss}, the loss metric converges about the same as for the batch normalized case. As expected, the convergence time in terms of number of iterations is slightly higher. One interesting section is how the 6 dB curve converges. It appears to have a strong section of downward concavity. It is possible that this occurred randomly, as the random number generator in Numpy was set to a random seed. It is also possible that because of an absence of batch normalization, some neurons saturated and did not change substantially for some time.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-nobatchnorm-loss}
\caption{Loss at various SNRs for Supervised Single-Layer Autoencoder without Batch Normalization at the Input}\label{fig:paris-loss}
\end{figure}

Somewhat unexpectedly, the MSE in Figure \ref{fig:paris-mse} converges to a lower value than that of Figure \ref{fig:paris-bn-mse}. This suggests that there may be an error in the simulation, likely in using the stored statistics for batch normalization as opposed to using an on-the-fly calculation of the minibatch statistics at inference time. However, we still achieve convergence here which is expected. Past 0 dB, the MSE seems to converge to a similar value, suggesting that the network has diminishing returns for higher SNR.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-nobatchnorm-mse}
\caption{MSE at various SNRs for Supervised Single-Layer Autoencoder without Batch Normalization at the Input}\label{fig:paris-mse}
\end{figure}


\subsection{Partitioned Autoencoder}

For the dense partitioned autoencoder, the loss function appears to converge although at a slower rate in Figure \ref{fig:dan-loss}. A rerun might use more than 2000 iterations. The loss function also converges to a higher magnitude value since the network is not supervised as well as the large regularization coefficients in the loss function.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/dan-dense-loss}
\caption{Loss at various SNRs for Single-Layer Partitioned Autoencoder\cite{stow}}
\label{fig:dan-loss}
\end{figure}

The MSE is surprisingly low in Figure \ref{fig:dan-mse}. Unlike in the supervised case, the MSE seems to spread out more as SNR increases. Even at 0 dB, the network seems to learn the noise to some success. A listening test indicates noticeably lower noise level with minimal introduced distortion.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/dan-dense-mse}
\caption{MSE at various SNRs for Single-Layer Partitioned Autoencoder\cite{stow}}
\label{fig:dan-mse}
\end{figure}

\subsection{Partitioned Curro Autoencoder}



\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/curro-loss}
\caption{Loss at various SNRs for Single-Layer Curro Autoencoder}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/curro-mse}
\caption{MSE at various SNRs for Single-Layer Curro Autoencoder}
\end{figure}

\subsection{Comparison of Loss Convergence}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss--6}
\caption{Loss Comparison of Various Networks at -6 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss--3}
\caption{Loss Comparison of Various Networks at -3 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-0}
\caption{Loss Comparison of Various Networks at 0 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-3}
\caption{Loss Comparison of Various Networks at 3 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-6}
\caption{Loss Comparison of Various Networks at 6 dB}
\end{figure}



\subsection{Comparison of Mean Squared Error Convergence}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse--6}
\caption{MSE Comparison of Networks at -6 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse--3}
\caption{MSE Comparison of Networks at -3 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-0}
\caption{MSE Comparison of Networks at 0 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-3}
\caption{MSE Comparison of Networks at 3 dB}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-6}
\caption{MSE Comparison of Networks at 6 dB}
\end{figure}

