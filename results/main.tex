We present results here for mainly shallow network architectures. At the output layer of each network, an identity nonlinearity is used. At any other layer, the modified ReLU (mReLU) is used. Unless otherwise noted, batch normalization is applied at the input layer. Each network is compared first to itself at varying noise levels (-6 dB, -3 dB, 0 dB, 3 dB, 6 dB SNR) in terms of convergence as well as the mean squared error (MSE) for inferences.

Training minibatches consist of 128 examples, each with 1024-sample FFT frames of $\norm{Y[k]}$ at a sampling rate 16 kHz. Time windows are windowed using the Hanning window, and we use 50\% overlap for perfect reconstruction at inference time. The examples used are a sum of sine waves at four fixed frequencies with uniform random amplitude and phase. The frequencies are chosen to form an A4 major chord (1-3-5-8) at slightly de-tuned frequencies so as not to allow the network to learn any pattern from the immediate harmonic structure.

\begin{align}
f &= [441, 549, 660, 881]\qquad \text{Hz}\\
x[n] &= \sum_{i=0}^{3} A_i \sin{2 \pi f_i / f_s n + \phi_i}, \qquad n=0\ldots ,1023\\
A_i &\sim U(0.25, 0.75)\\
\phi &\sim U(0, 2\pi)
\end{align}

Applied noise is additive-white Gaussian noise (AWGN), with the variance $\sigma^2$ selected to achieve the desired average SNR for each minibatch as in Equation \ref{eq:siggy}.

\begin{align}
N[n] &\sim N(0, \sigma^2), \qquad n=0\ldots ,1023\\
y[n] &= x[n] + N[n]
\end{align}

In semi-supervised cases where we use the soft label $y$ for noise-only versus signal-plus-noise examples, we use 25\% noise-only examples per minibatch. For inference calculations, we construct a minibatch with consecutive overlapping, windowed frames.

Simulations are written in Python 2.7 using Lasasgne \cite{sander_dieleman_2015_27878}, a ``lightweight library to build and train neural networks in Theano.'' Theano is a ``Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently.'' \cite{2016arXiv160502688short} Theano boasts parallel GPU support, numpy support (a mathematical Python library), numerical stability, and symbolic differentiation, among other features. These libraries and frameworks allow for ease of developing deep, novel architectures and save time in doing things like calculating gradients, weight updates and back-propagation. Sample simulation code is shown in the Appendix.

Weight updates are calculated using Adam updates \cite{DBLP:journals/corr/KingmaB14}. 2000 iterations (minibatches) are used for each simulation. Unless otherwise noted, each hidden layer uses 2000 hidden nodes.

Loss-based plots show the loss function convergence during training iterations, where the loss function is as defined in the previous section for each network architecture. Mean squared error plots show the MSE every 50 training examples for an inference example that does not change.

\subsection{Supervised Autoencoder}

The following results show a single-layer autoencoder with and without batch normalization at the input layer. We present these to compare the effects of batch normalization as well as to show the differences between supervised and semi-supervised de-noising approaches.

\subsubsection{Batch Normalized Input}

In Figure \ref{fig:paris-bn-loss}, we can see that the loss function appears to converge at or before 2000 iterations. As expected, as SNR increases, the loss objective converges to a lower value. Since this network is trained using only the squared error loss, this should be expected. Note that as the SNR increases, the difference in the converged value gets smaller. Also interesting is the fact that lower SNR plots converge more quickly but to higher values. This suggests that the network does not respond well to too much noise.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-loss}
\caption{Loss at various SNRs for Supervised Single-Layer Autoencoder with Batch Normalization at the Input}\label{fig:paris-bn-loss}
\end{figure}

In Figure \ref{fig:paris-bn-mse}, we can see that the MSE generally goes down as SNR goes up. This should be expected, though perhaps there may be an error in the simulation since the lines blur a bit between -3 dB and 6 dB.

We also note from personal listening tests that the reconstructed signals have some distortion introduced from the network.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-mse}
\caption{MSE at various SNRs for Supervised Single-Layer Autoencoder with Batch Normalization at the Input}\label{fig:paris-bn-mse}
\end{figure}

\subsubsection{Non-Batch Normalized Input}

As expected, in Figure \ref{fig:paris-loss}, the loss metric converges about the same as for the batch normalized case. As expected, the convergence time in terms of number of iterations is slightly higher. One interesting section is how the 6 dB curve converges. It appears to have a strong section of downward concavity. It is possible that this occurred randomly, as the random number generator in Numpy was set to a random seed. It is also possible that because of an absence of batch normalization, some neurons saturated and did not change substantially for some time.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-nobatchnorm-loss}
\caption{Loss at various SNRs for Supervised Single-Layer Autoencoder without Batch Normalization at the Input}\label{fig:paris-loss}
\end{figure}

The MSE in Figure \ref{fig:paris-mse} converges to a lower value than that of Figure \ref{fig:paris-bn-mse}. This suggests that there may be an error in the simulation, likely in using the stored statistics for batch normalization as opposed to using an on-the-fly calculation of the minibatch statistics at inference time. Batch normalization usually allows for faster convergence of the loss function. This may suggest that the mean squared error of the magnitude FFT coefficients are not as directly correlated to the time-domain signal mean squared error convergence. However, we still achieve convergence here which is expected. Past 0 dB, the MSE seems to converge to a similar value, suggesting that the network has diminishing returns for higher SNR.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/paris-nobatchnorm-mse}
\caption{MSE at various SNRs for Supervised Single-Layer Autoencoder without Batch Normalization at the Input}\label{fig:paris-mse}
\end{figure}


\subsection{Partitioned Autoencoder}

For the dense partitioned autoencoder, the loss function appears to converge although at a slower rate in Figure \ref{fig:dan-loss}. The loss function also converges to a higher magnitude value since the network is not supervised. In addition, the large regularization term in the loss function defined in Equation \ref{eq:stowloss} contributes to the higher convergence values for the simulation. 

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/dan-dense-loss}
\caption{Loss at various SNRs for Single-Layer Partitioned Autoencoder\cite{stow}}
\label{fig:dan-loss}
\end{figure}

The MSE is surprisingly low in Figure \ref{fig:dan-mse}. Unlike in the supervised case, the MSE seems to spread out more as SNR increases. Even at 0 dB, the network seems to learn the noise to some success. A listening test indicates noticeably lower noise level with minimal introduced distortion.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/dan-dense-mse}
\caption{MSE at various SNRs for Single-Layer Partitioned Autoencoder\cite{stow}}
\label{fig:dan-mse}
\end{figure}

\subsection{Partitioned Curro Autoencoder}

For the Curro Autoencoder simulation, we used 2 dense layers of size 2048 each before partitioning into two 1024 networks. After the partition, we used two dense layers and an output layer for both partitions, all at size 1024.

Interesting to note for the Curro Autoencoder is the fact that it converges almost as quickly across SNR, which can be seen in Figure \ref{fig:curro-loss}. This suggests that the network might have a lesser dependence on SNR than for the other networks. For a real-time systems application, this suggests that the Curro network could outperform the Dense Partitioned Autoencoder. On the other hand, this could suggest that the network quickly gets stuck in a local minimum and fails to reach lower convergence.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/curro-loss}
\caption{Loss at various SNRs for Single-Layer Curro Autoencoder}\label{fig:curro-loss}
\end{figure}

In Figure \ref{fig:curro-mse}, for -6 dB, the time-domain MSE seems to go up slightly after 50-100 iterations, suggesting that either the loss function or the reconstruction could be incorrect. As expected, the network performs better for higher SNR but still seems to get diminishing returns for SNRs greater than 0 dB.

The plot shows the MSE for reconstruction using only the signal half of the network, i.e. the top half. Interestingly, a listening test indicates that summing the two partitions at the output seems to produce a lower noise volume and lower distortion than for either partition individually. This suggests that the network might not be properly partitioning. It is currently unclear as to whether or not this is the result of a bug in the code or a fundamental flaw in the network architecture. Since this network does not have additional dense layers at the summed output, it could be that the network needs to be deeper.

Another possibility is that the underlying symmetry in the FFT is causing the network to effectively initialize to two parallel networks. This could be mitigated by only using the first half of the FFT. Also, adding batch normalization to the rest of the layers might result in better convergence and performance.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/curro-mse}
\caption{MSE at various SNRs for Single-Layer Curro Autoencoder}\label{fig:curro-mse}
\end{figure}

\subsection{Comparison of Loss Convergence}

A comparison of loss functions is not reasonable 
Since our networks have different loss functions, it does not make sense to compare them based on the converged value. Rather, we would like to compare the networks for convergence time in terms of number of iterations, i.e. number of minibatches exposed to the network. From previous figures, we should expect that the SNR might have some difference depending on the network.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss--6}
\caption{Loss Comparison of Various Networks at -6 dB}\label{fig:comp-loss--6}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss--3}
\caption{Loss Comparison of Various Networks at -3 dB}\label{fig:comp-loss--3}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-0}
\caption{Loss Comparison of Various Networks at 0 dB}\label{fig:comp-loss-0}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-3}
\caption{Loss Comparison of Various Networks at 3 dB}\label{fig:comp-loss-3}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-loss-6}
\caption{Loss Comparison of Various Networks at 6 dB}\label{fig:comp-loss-6}
\end{figure}

In Figures \cref{fig:comp-loss--6,fig:comp-loss--3,fig:comp-loss-0,fig:comp-loss-3,fig:comp-loss-6}, the Curro Autoencoder seems to converge the quickest, followed by the two supervised networks, then the Dense Partitioned Autoencoder converging the slowest. As expected, as SNR goes up, the supervised networks converge slower while the semi-supervised networks seem to not be as effected by SNR.


\subsection{Comparison of Mean Squared Error Convergence}

In terms of MSE convergence, we can safely compare both the convergence time and the value of convergence since the reconstruction is based on the same inference example.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse--6}
\caption{MSE Comparison of Networks at -6 dB}\label{fig:comp-mse--6}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse--3}
\caption{MSE Comparison of Networks at -3 dB}\label{fig:comp-mse--3}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-0}
\caption{MSE Comparison of Networks at 0 dB}\label{fig:comp-mse-0}
\end{figure}

In Figures \cref{fig:comp-mse--6,fig:comp-mse--3,fig:comp-mse-0}, we see that at low SNR the Dense Partitioned Autoencoder usually has the highest MSE. We expect the supervised autoencoders to have lower MSE, which is the case for these figures. However, the Curro Autoencoder has better performance than the supervised systems which is somewhat unexpected. We should expect that the network which has access to the ground truth during training should converge to a lower MSE. This could be a simulation error and is an important area of future research.

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-3}
\caption{MSE Comparison of Networks at 3 dB}\label{fig:comp-mse-3}
\end{figure}

\begin{figure}[!ht]
\centering
\includegraphics[width=.8\textwidth]{../thesis/thesis/comparisons/plotfinal/pdf/comparison-mse-6}
\caption{MSE Comparison of Networks at 6 dB}\label{fig:comp-mse-6}
\end{figure}

Also interesting to note is that for higher SNR, the Partitioned Dense Autoencoder outperforms both supervised methods. This can be seen in Figures \cref{fig:comp-mse-3,fig:comp-mse-6}. If the results are correct, this suggests that a semi-supervised network can outperform a supervised network. This might be the result of enforcing a structure of the latent space, whereas normally differences in initialization of parameters can cause vastly different latent representations.

Also interesting is the fact that for the highest SNR 6 dB, the Partitioned Dense Autoencoder outperforms the Curro Autoencoder. This can be seen in Figure \ref{fig:comp-mse-6}. Again, this could be the result of a simulation error or it could suggest that the Dense Partitioned Autoencoder performs well at high SNR.
