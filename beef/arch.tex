% first section should be classic supervised auto-encoder and maybe paris auto-encoder. then the next section can be dan stowells thing, and then finally the curro-net.
In the following sections, we detail all considered shallow network architectures. Note that these network architectures can easily be extended to deep networks by adding corresponding encode and decode layers before and after the latent representation, respectively. For the purposes of the results, we consider FFT 

\subsection{Supervised Autoencoder}


\subsection{Partitioned Autoencoder}

\subsubsection{Phase Reconstruction}

\subsection{Curro Autoencoder}


