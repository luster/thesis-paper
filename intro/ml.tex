% explain ML basics
Machine learning involves the use of computer algorithms to make decisions based on training data. Generally, this falls into categorizing input data (classification) or determining a mathetmatical function to determine a continuous output given an input (regression). Popular classification examples include recognizing handwritten digits as well as determining whether an image contains a cat or a dog. An example of a regression problem is determining the temperature given a set of input features (humidity, latitude, longitude, date, etc.).

Problems where training data contain input data vectors as well as the correct output vectors (targets) are known as supervised learning problems. Training a model to denoise audio where noise was introduced to the clean audio would be a supervised learning problem. On the other hand, training a model to denoise audio where the underlying clean signal is not known is an unsupervised learning problem. Different loss (objective) functions and neural network architectures can be exploited to accomplish denoising without the clean data.

For the purposes of this thesis, we use machine learning to determine an underlying nonlinear function that removes noise from time slices of audio (i.e. regression). These slices can then be pieced back together through overlap-add resynthesis. To clarify, this is a general linear model that maps an input noisy audio vector $y[n]=x[n]+N[n]$ to $\tilde{x}[n]$, a target denoised audio vector, where $x[n]$ is the underlying clean signal and $N[n]$ is the additive background noise.

\subsubsection{Regression}
A classical regression technique is linear regression, where one or more independent variables $x_{i}$ are used to determine a scalar dependent variable $y$. The case of a single independent variable $x$ is known as simple linear regression. More formally, for $k$ independent variables, we would like to determine a weight vector $\vec{w}$ and bias vector $\vec{b}$:

\begin{align}
y_i &= w_{1}x_{i1} + \cdots + w_{k}x_{ik} + b_{i}, \qquad i=1\ldots ,n \\
\vec{y} &= \vec{x^T}\vec{w} + \vec{b}
\end{align}

where the rows of $x^{T}$ are the example input observations and $\vec{y}$ and $\vec{b}$ are column vectors.

By extension, the case of linearly estimating a vector output giving a vector input is known as a generalized linear model. A canonical example would be estimating a sine wave $x[n]$ over some number of $N$ samples given noisy samples $y[n]=x[n]+N[n]$.


% \subsubsection{Overfitting and Curse of Dimensionality}


% \subsubsection{Loss functions and Regularization}


% \subsubsection{Gradient Stuff?}

